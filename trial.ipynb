{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --quiet openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --quiet  PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade --quiet  langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --quiet langchain-openai faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --quiet unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --quiet  pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --quiet  pdfminer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "# api_key= os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from langchain.document_loaders import TextLoader\n",
    "# from langchain.document_loaders import PyPDFLoader\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "# from operator import itemgetter\n",
    "# from langchain_community.vectorstores import FAISS\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "# from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader, TextLoader\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain.vectorstores import Chroma, Pinecone\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# from langchain.retrievers import ParentDocumentRetriever\n",
    "# from langchain.storage import InMemoryStore\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "# from langchain_community.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.retrievers import ParentDocumentRetriever\n",
    "# from langchain.storage import InMemoryStore\n",
    "# from langchain_community.document_loaders import TextLoader\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "# from PyPDF2 import PdfReader\n",
    "# from langchain.docstore.document import Document\n",
    "\n",
    "# def load_pdf(file_path, start_page=10):\n",
    "#     with open(file_path, 'rb') as file:\n",
    "#         pdf_reader = PdfReader(file)\n",
    "#         text = ''\n",
    "#         for page_number, page in enumerate(pdf_reader.pages):\n",
    "#             if page_number + 1 >= start_page:  # Start reading from the 10th page\n",
    "#                 text += page.extract_text()\n",
    "#         # Remove emails from the text\n",
    "#         text_without_emails = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
    "#         # Ensure text is Unicode supported\n",
    "#         unicode_text = text_without_emails.encode('utf-8', 'ignore').decode('utf-8')\n",
    "#         return [Document(page_content=unicode_text, metadata={'file_name': os.path.basename(file_path)})]\n",
    "\n",
    "# pdf_directory = \"Must-Read-Books-for-Financial-Professionals\"  # Replace this with the path to your directory containing PDF files\n",
    "# pdf_files = [os.path.join(pdf_directory, file) for file in os.listdir(pdf_directory) if file.endswith('.pdf')]\n",
    "\n",
    "# docs = []\n",
    "# for pdf_file in pdf_files:\n",
    "#     docs.extend(load_pdf(pdf_file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size = 2000, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \" \"\n",
    "# for i in docs:\n",
    "#     text = text + \"\\n\" + i.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# chunks_splitted = text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DEEL\\Desktop\\Priyanshu\\Ollama\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\DEEL\\Desktop\\Priyanshu\\Ollama\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DEEL\\.cache\\huggingface\\hub\\models--Muennighoff--SGPT-125M-weightedmean-nli-bitfit. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Get our models - The package will take care of downloading the models automatically\n",
    "# For best performance: Muennighoff/SGPT-5.8B-weightedmean-nli-bitfit\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Muennighoff/SGPT-125M-weightedmean-nli-bitfit\")\n",
    "model = AutoModel.from_pretrained(\"Muennighoff/SGPT-125M-weightedmean-nli-bitfit\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings(text,model= model , tokenizer= tokenizer):\n",
    "    batch_tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**batch_tokens, output_hidden_states=True, return_dict=True).last_hidden_state\n",
    "    weights = (\n",
    "    torch.arange(start=1, end=last_hidden_state.shape[1] + 1)\n",
    "    .unsqueeze(0)\n",
    "    .unsqueeze(-1)\n",
    "    .expand(last_hidden_state.size())\n",
    "    .float().to(last_hidden_state.device))\n",
    "    input_mask_expanded = (\n",
    "    batch_tokens[\"attention_mask\"]\n",
    "    .unsqueeze(-1)\n",
    "    .expand(last_hidden_state.size())\n",
    "    .float())\n",
    "    sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded * weights, dim=1)\n",
    "    sum_mask = torch.sum(input_mask_expanded * weights, dim=1)\n",
    "    \n",
    "    embeddings = sum_embeddings / sum_mask\n",
    "    return embeddings.detach().cpu().numpy()[0].tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial = chunks_splitted[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p=[]\n",
    "# c=1\n",
    "# for i in chunks_splitted:\n",
    "#     x={\"id\": str(c), \"values\": embeddings(i.page_content), \"metadata\":{\"text\": i.page_content}}\n",
    "#     p.append(x)\n",
    "#     c=c+1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dotenv\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "# from pinecone import Pinecone\n",
    "# pinecone_api_key = os.getenv(\"pinecone_api_key\")\n",
    "# pc = Pinecone(api_key = pinecone_api_key)\n",
    "# index = pc.Index(\"finance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from load_pinecone_index import index \n",
    "# def upsert_data_in_pinecone(dict, index):\n",
    "#     index.upsert(vectors=[dict], namespace=\"Fianl_set1\")\n",
    "\n",
    "# for i in range(len(p)):\n",
    "#     upsert_data_in_pinecone(p[i], index= index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "cohere_api_key= os.getenv(\"cohere_api_key\")\n",
    "co = cohere.Client(cohere_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from load_pinecone_index import index\n",
    "def retriever_from_pinecone_then_reranking(query, index = index, embeddings= embeddings):\n",
    "    embed_query = embeddings(query)\n",
    "    pc = index.query(vector = embed_query,top_k=10, include_values = False, include_metadata = True, namespace=\"Fianl_set1\")\n",
    "    id_from_docs =[]\n",
    "    text_retrieved =[]\n",
    "    for i in pc[\"matches\"]:\n",
    "        id_from_docs.append(i[\"id\"])\n",
    "    for i in pc[\"matches\"]:\n",
    "        text_retrieved.append(i[\"metadata\"][\"text\"])\n",
    "    rerank_docs = co.rerank(query=query, documents=text_retrieved, top_n=5, model=\"rerank-english-v2.0\")\n",
    "    text =\"\"\n",
    "    text = text +\"\\n\"+  rerank_docs[0].document[\"text\"]\n",
    "    text = text + \"\\n\"+  rerank_docs[1].document[\"text\"]\n",
    "    text = text + \"\\n\"+  rerank_docs[2].document[\"text\"]\n",
    "    text = text +\"\\n\"+  rerank_docs[3].document[\"text\"]\n",
    "    text = text +\"\\n\"+  rerank_docs[4].document[\"text\"]\n",
    "    response = co.summarize(\n",
    "    text=text,\n",
    "    model='command',\n",
    "    length='long',\n",
    "    extractiveness='high')\n",
    "    return response.summary\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"No technique, no matter how sophisticated, can consistently predict the future. That conclusion, if correct, represents to me (and I hope to you) a resounding confirmation of the random-walk approach. Recall that the weak form of the efficient-market hypothesis (the random-walk notion) says simply that the technical analysis of past price patterns to forecast the future is useless because any information from such an analysis will already have been incorporated in current market prices. If today's direction \\x97 up or down, forward or backward \\x97 does indeed predict tomorrow's step, then you will act on it today rather than tomorrow. Thus, if market participants were confident that the price of any security would double next week, the price would not reach that level over five working days.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_from_pinecone_then_reranking(\"what is random walk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_builder(retieved_Context, user_query):\n",
    "    input_json = {\"user_query\": user_query, \"context\": retieved_Context}\n",
    "    s=\"\"\" Attached below is the json format which consists of user_query and conext of the user_query which contain some retrieved information from literature regarding user_query, based on that context you will answer the user query. input json is {}\"\"\".format(input_json)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_builder(retriever_from_pinecone_then_reranking(\"what is random walk\"), \"what is random walk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chats():\n",
    "    p=0\n",
    "    chat_history = [\n",
    "\t{\"user_name\": \"Chatbot\", \"text\": \"Hey! How can I help you today?\"},]\n",
    "    while True and p<5:\n",
    "        input_query = input()\n",
    "        input_query= query_builder(retriever_from_pinecone_then_reranking(input_query), input_query)\n",
    "        response = co.chat(message=input_query, chat_history=chat_history)\n",
    "        x={\"user_name\":\"user\", \"text\":input_query}\n",
    "        chat_history.append(x)\n",
    "        chat_history.append({\"user_name\": \"Chatbot\", \"text\":response.text})\n",
    "        print(response.text)\n",
    "        p=p+1\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's proceed using the provided context! Feel free to ask me any questions you may have regarding the meaning of random walks, and I will do my best to provide comprehensive answers.\n",
      "Okay, the concept of a random walk is a theoretical model used to explain the behaviour of a process characterised by non-deterministic(random) movement influenced by external factors.\n",
      "\n",
      "In the context of your provided text snippet, it seems that the random walk is being used as a metaphor to describe the behaviour of financial markets and the uncertainties faced by investors. The writer suggests that it is impossible to predict the direction of market trends with certainty, and any analysis focused on predicting future market movements based on historical data is futile. This idea is re-emphasized through the phrase \"if today's direction (up or down, forward or backward) does indeed predict tomorrow's step, then you will act on it today rather than tomorrow.\" Here, the writer alludes to the concept of a random walk, where the movement of the market cannot be predicted merely based on the current trajectory, and tomorrow's steps are independent of today's. \n",
      "\n",
      "Therefore, the text supports the idea of embarking on investments based on current information and available insights, rather than waiting for potential future developments. This is aligned with the concept of a random walk, where one cannot reliably predict market behaviour, and the necessary actions would be to act in the present based on current knowledge and opportunities. \n",
      "Let me know if there is anything else I can help you with!\n"
     ]
    }
   ],
   "source": [
    "chats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.chat import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks for the context! Here is my answer based on the information provided:\n",
      "\n",
      "Financial engineering is a field that looks to optimize financial strategies and activities by combining financial theories and methodologies with mathematical and statistical models. It involves creating and using different financial instruments with the goal of maximizing profits and minimizing risks. Financial engineering is an evolving field that seeks to find innovative ways to manage and utilize money.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
